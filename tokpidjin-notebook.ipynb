{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":67357,"databundleVersionId":8951125,"sourceType":"competition"},{"sourceId":91496,"databundleVersionId":11802066,"sourceType":"competition"},{"sourceId":13350375,"sourceType":"datasetVersion","datasetId":7010692}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\n!echo $HOME\n!pwd\n!uname -a\n!cat /etc/debian_version\n!python --version\n\n!python -m cupyx.tools.install_library --library cudnn --cuda 12.x\n!ls -ld /root/.cupy/cuda_lib/12.x/cudnn/8.8.1\n!ls -ld /root/.cupy/cuda_lib/12.x/cudnn/8.8.1/lib/libcudnn_cnn_infer.so.8.8.1\n!cp -R /root/.cupy /kaggle/working\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !nvidia-smi\n# !nvcc --version","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T10:48:48.643872Z","iopub.execute_input":"2025-10-09T10:48:48.644135Z","iopub.status.idle":"2025-10-09T10:48:48.960375Z","shell.execute_reply.started":"2025-10-09T10:48:48.644114Z","shell.execute_reply":"2025-10-09T10:48:48.959419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# To reinstall cudnn, enable internet and uncomment the next 3 lines\n# !rm -rf /root/.cupy\n# !python -m cupyx.tools.install_library --library cudnn --cuda 12.x\n# !cp -R /root/.cupy /kaggle/working\n\n!rm -rf /root/.cupy\n!cp -R /kaggle/working/.cupy /root\n!cp -R /kaggle/input/arc-prize-2025 /kaggle/working\n!cp -R /kaggle/input/tokpidjin      /kaggle/working\n\n%cd /kaggle/working/tokpidjin\n\n!bash run_card.sh -o -i -b -c -3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T13:26:29.967810Z","iopub.execute_input":"2025-10-12T13:26:29.968404Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/tokpidjin\nInitial run - removing old solvers\nCuPy GPU support enabled for Kaggle\nTesting GPU acceleration...\nCuPy GPU support enabled for Kaggle\nKaggle GPU Support: True (4 devices)\n  GPU 0: Compute 89, Memory: 22.3GB\n  GPU 1: Compute 89, Memory: 22.3GB\n  GPU 2: Compute 89, Memory: 22.3GB\n  GPU 3: Compute 89, Memory: 22.3GB\n✓ Kaggle GPU Optimizer initialized\nGPU Available: True\nGPU 0 configured:\n  Total memory: 22.28GB\n  Available: 22.09GB\n  Pool limit: 17.82GB\nGPU memory configured\nGPU detected, enabling GPU acceleration\nSun Oct 12 01:27:16 PM UTC 2025\n-- One run only --\nCuPy GPU support enabled for Kaggle\ncard.py:592: len(all_solvers) = 346\ncard.py:259: item = 'DOWN_LEFT'\ncard.py:259: item = 't211'\ncard.py:259: item = 'width_o'\ncard.py:259: item = 'NINE'\ncard.py:259: item = 't255'\ncard.py:259: item = 'TWO_BY_ZERO'\nCuPy GPU support enabled for Kaggle\nKaggle GPU Support: True (1 devices)\n  GPU 0: Compute 89, Memory: 22.3GB\n✓ Kaggle GPU Optimizer initialized\nBatt GPU: Enabled (1 GPU, KaggleGPUOptimizer)\nrun_batt.py:543: -- 9c56f360 - 0 start --\nrun_batt.py:389: -- 9c56f360 - 0 --\nrun_batt.py:410: demo[0] - 9c56f360 - 32\nrun_batt.py:410: demo[1] - 9c56f360 - 32\nrun_batt.py:410: demo[2] - 9c56f360 - 32\nrun_batt.py:460: test[0] - 9c56f360 - 32\nrun_batt.py:548: -- 9c56f360 - 0 scored --\nrun_batt.py:553: -- 9c56f360 - 0 done - 110 candidates scored\nrun_batt.py:543: -- 67c52801 - 1 start --\nrun_batt.py:389: -- 67c52801 - 1 --\nrun_batt.py:410: demo[0] - 67c52801 - 32\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"'''\n# let's try to make a submission\n\nsubmission = dict()\n# iterate over all tasks\nfor key, task in tqdm.tqdm(test_challenges.items()):\n    train_inputs = [example['input'] for example in task['train']]\n    train_outputs = [example['output'] for example in task['train']]\n    hypotheses = []\n    # iterate over all programs\n    for program_string, program in programs.items():\n        try:\n            if all([program(i) == o for i, o in zip(train_inputs, train_outputs)]):\n                # remember program if it explains all training examples\n                hypotheses.append(program_string)\n        except:\n            pass\n    # select first program for making predictions\n    predictions = [example['input'] for example in task['test']]\n    if len(hypotheses) > 0:\n        print(f'found {len(hypotheses)} candidate programs for task {key}!')\n        program_string = hypotheses[0]\n        program = eval(program_string)\n        try:\n            predictions = [program(example['input']) for example in task['test']]\n        except:\n            pass\n    submission[key] = [{'attempt_1': grid, 'attempt_2': grid} for grid in predictions]\nprint(f'\\nMade guesses for {len(guesses)} tasks')\n\nwith open('submission.json', 'w') as fp:\n    json.dump(submission, fp)\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}