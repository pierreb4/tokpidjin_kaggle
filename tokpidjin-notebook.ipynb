{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":67357,"databundleVersionId":8951125,"sourceType":"competition"},{"sourceId":91496,"databundleVersionId":11802066,"sourceType":"competition"},{"sourceId":13350375,"sourceType":"datasetVersion","datasetId":7010692}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\n!echo $HOME\n!pwd\n!uname -a\n!cat /etc/debian_version\n!python --version\n\n!python -m cupyx.tools.install_library --library cudnn --cuda 12.x\n!ls -ld /root/.cupy/cuda_lib/12.x/cudnn/8.8.1\n!ls -ld /root/.cupy/cuda_lib/12.x/cudnn/8.8.1/lib/libcudnn_cnn_infer.so.8.8.1\n!cp -R /root/.cupy /kaggle/working\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !nvidia-smi\n# !nvcc --version","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T10:48:48.643872Z","iopub.execute_input":"2025-10-09T10:48:48.644135Z","iopub.status.idle":"2025-10-09T10:48:48.960375Z","shell.execute_reply.started":"2025-10-09T10:48:48.644114Z","shell.execute_reply":"2025-10-09T10:48:48.959419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# To reinstall cudnn, enable internet and uncomment the next 3 lines\n# !rm -rf /root/.cupy\n# !python -m cupyx.tools.install_library --library cudnn --cuda 12.x\n# !cp -R /root/.cupy /kaggle/working\n\n!rm -rf /root/.cupy\n!cp -R /kaggle/working/.cupy /root\n!cp -R /kaggle/input/arc-prize-2025 /kaggle/working\n!cp -R /kaggle/input/tokpidjin      /kaggle/working\n\n%cd /kaggle/working/tokpidjin\n\n# !bash -x run_card.sh -o -i -b -c -32\n# !python run_batt.py --timing -i 007bbfb7 -b tmp_batt_onerun_run 2>&1 | tail -50\n# !python run_batt.py --timing -i 007bbfb7 -b tmp_batt_onerun_run\n# !python -u run_batt.py --timing -i -t 1.0 -c -10 -b tmp_batt_onerun_run\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T09:18:53.896096Z","iopub.execute_input":"2025-10-13T09:18:53.896805Z","iopub.status.idle":"2025-10-13T09:18:59.177537Z","shell.execute_reply.started":"2025-10-13T09:18:53.896778Z","shell.execute_reply":"2025-10-13T09:18:59.176946Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/tokpidjin\n======================================================================\nQUICK GPU BENCHMARK\n======================================================================\n\n[1/3] Sequential...\nMultiGPUOptimizer initialized with 4/4 GPUs\nMultiGPUOptimizer initialized with 4/4 GPUs\nBatt GPU: Enabled (4 GPUs, MultiGPUOptimizer)\nTime: 0.627s | Throughput: 31.9 samples/s\n\n[2/3] Parallel CPU...\nTime: 0.210s | Throughput: 95.1 samples/s | Speedup: 2.98x\n\n[3/3] Parallel GPU...\nMultiGPUOptimizer initialized with 4/4 GPUs\nMultiGPUOptimizer initialized with 4/4 GPUs\nTime: 0.209s | Throughput: 95.5 samples/s | Speedup: 2.99x\n\n======================================================================\nRESULTS SUMMARY\n======================================================================\nMode                 Time       Speedup   \n----------------------------------------------------------------------\nSequential              0.627s      1.00x\nParallel CPU            0.210s      2.98x\nParallel GPU            0.209s      2.99x\n======================================================================\n\n‚ùå BELOW TARGET: GPU speedup 3.0x < 5x\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"'''\n# let's try to make a submission\n\nsubmission = dict()\n# iterate over all tasks\nfor key, task in tqdm.tqdm(test_challenges.items()):\n    train_inputs = [example['input'] for example in task['train']]\n    train_outputs = [example['output'] for example in task['train']]\n    hypotheses = []\n    # iterate over all programs\n    for program_string, program in programs.items():\n        try:\n            if all([program(i) == o for i, o in zip(train_inputs, train_outputs)]):\n                # remember program if it explains all training examples\n                hypotheses.append(program_string)\n        except:\n            pass\n    # select first program for making predictions\n    predictions = [example['input'] for example in task['test']]\n    if len(hypotheses) > 0:\n        print(f'found {len(hypotheses)} candidate programs for task {key}!')\n        program_string = hypotheses[0]\n        program = eval(program_string)\n        try:\n            predictions = [program(example['input']) for example in task['test']]\n        except:\n            pass\n    submission[key] = [{'attempt_1': grid, 'attempt_2': grid} for grid in predictions]\nprint(f'\\nMade guesses for {len(guesses)} tasks')\n\nwith open('submission.json', 'w') as fp:\n    json.dump(submission, fp)\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}