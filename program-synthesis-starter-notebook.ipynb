{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaL4","dataSources":[{"sourceId":67357,"databundleVersionId":8951125,"sourceType":"competition"},{"sourceId":91496,"databundleVersionId":11802066,"sourceType":"competition"},{"sourceId":13312223,"sourceType":"datasetVersion","datasetId":7010692}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\n!echo $HOME\n!pwd\n!uname -a\n!cat /etc/debian_version\n!python --version\n\n!python -m cupyx.tools.install_library --library cudnn --cuda 12.x\n!ls -ld /root/.cupy/cuda_lib/12.x/cudnn/8.8.1\n!ls -ld /root/.cupy/cuda_lib/12.x/cudnn/8.8.1/lib/libcudnn_cnn_infer.so.8.8.1\n!cp -R /root/.cupy /kaggle/working\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !nvidia-smi\n# !nvcc --version","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T10:48:48.643872Z","iopub.execute_input":"2025-10-09T10:48:48.644135Z","iopub.status.idle":"2025-10-09T10:48:48.960375Z","shell.execute_reply.started":"2025-10-09T10:48:48.644114Z","shell.execute_reply":"2025-10-09T10:48:48.959419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !python -m cupyx.tools.install_library --library cudnn --cuda 12.x\n# !cp -R /root/.cupy /kaggle/working\n\n!ls -ld /kaggle/working/.cupy\n!cp -R /kaggle/working/.cupy /root\n\n# !ls -l /root/.cupy/cuda_lib/12.x/cudnn/8.8.1/include\n# !sudo cp -P /kaggle/working/.cupy/cuda_lib/12.x/cudnn/8.8.1/include/* /usr/local/cuda-12.5/include/\n\n# !ls -l /root/.cupy/cuda_lib/12.x/cudnn/8.8.1/lib\n# !sudo cp -P /kaggle/working/.cupy/cuda_lib/12.x/cudnn/8.8.1/lib/* /usr/local/cuda-12.5/lib64/\n\n# !ls -ld /root/.cupy/cuda_lib/12.x/cudnn/8.8.1/lib/libcudnn_cnn_infer.so.8.8.1\n# !(export LD_LIBRARY_PATH=/root/.cupy/cuda_lib/12.x/cudnn/8.8.1/lib:$LD_LIBRARY_PATH; echo $LD_LIBRARY_PATH; sudo ldconfig)\n\n!cp -R /kaggle/input/arc-prize-2025 /kaggle/working\n!cp -R /kaggle/input/tokpidjin      /kaggle/working\n# !(cd tokpidjin; python test_kaggle_gpu_optimized.py)\n!(cd tokpidjin; bash run_card.sh -o -i -b -c -32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T08:16:47.245927Z","iopub.execute_input":"2025-10-10T08:16:47.246184Z","iopub.status.idle":"2025-10-10T08:16:57.161411Z","shell.execute_reply.started":"2025-10-10T08:16:47.246145Z","shell.execute_reply":"2025-10-10T08:16:57.160503Z"}},"outputs":[{"name":"stdout","text":"drwxr-xr-x 3 root root 4096 Oct 10 08:15 /kaggle/working/.cupy\n======================================================================\nKaggle GPU Optimized Batch Processing Test\n======================================================================\nCuPy GPU support enabled for Kaggle\nKaggle GPU Support: True (2 devices)\n  GPU 0: Compute 75, Memory: 14.7GB\n  GPU 1: Compute 75, Memory: 14.7GB\n✓ Kaggle GPU Optimizer initialized\n✓ All imports successful\n✓ GPU Available: True\n✓ GPU Count: 2\n  GPU 0: 14.7GB total, 14.6GB free\n  GPU 1: 14.7GB total, 14.6GB free\n\n======================================================================\nTest 1: Optimized Batch Processing Benchmark\n======================================================================\n\nWarming up GPU (triggering JIT compilation for all operations)...\nWarmup complete (all kernels compiled)\n\n============================================================\nGPU Batch Processing Benchmark (Kaggle Optimized)\n============================================================\n\nWarming up GPU (JIT compilation)...\nWarmup complete\n\n\nBatch size: 10\n  CPU: 1.83ms\n  GPU: 0.15ms\n  Speedup: 11.85x ✓\n  Correctness: ✓\n\nBatch size: 50\n  CPU: 0.91ms\n  GPU: 0.63ms\n  Speedup: 1.45x ✓\n  Correctness: ✓\n\nBatch size: 100\n  CPU: 1.62ms\n  GPU: 0.84ms\n  Speedup: 1.94x ✓\n  Correctness: ✓\n\nBatch size: 200\n  CPU: 3.20ms\n  GPU: 1.29ms\n  Speedup: 2.48x ✓\n  Correctness: ✓\n\n======================================================================\nTest 2: KaggleGPUOptimizer - Real DSL-like Operations\n======================================================================\n\nSmall batch (20 grids, 25x25):\n  CPU:        1.69ms\n  GPU:        1.19ms\n  Speedup:    1.43x ✓\n  Results: 20 grids processed ✓\n\nMedium batch (50 grids, 25x25):\n  CPU:        3.83ms\n  GPU:        0.63ms\n  Speedup:    6.10x ✓\n  Results: 50 grids processed ✓\n\nLarge batch (100 grids, 25x25):\n  CPU:        7.13ms\n  GPU:        0.87ms\n  Speedup:    8.19x ✓\n  Results: 100 grids processed ✓\n\nVery large batch (200 grids, 25x25):\n  CPU:       14.04ms\n  GPU:        1.45ms\n  Speedup:    9.69x ✓\n  Results: 200 grids processed ✓\n\n======================================================================\nTest 3: Pipeline Operations (Multiple ops without CPU transfer)\n======================================================================\nPipeline: 3 operations on 100 grids\n  CPU:        1.28ms\n  GPU:        0.62ms\n  Speedup:    2.05x ✓\n  Results: 100 grids ✓\n\n======================================================================\nTest 4: GPUBatchProcessor Integration\n======================================================================\nUsing KaggleGPUOptimizer (batch_size=64)\n✓ GPUBatchProcessor initialized\n  Using optimized Kaggle GPU: True\n\n======================================================================\nSummary\n======================================================================\n\n✓ Kaggle GPU optimization complete\n\nKey improvements:\n1. Minimum batch size threshold (20+) to avoid GPU overhead\n2. Single GPU memory allocation per batch (not per grid)\n3. All operations stay on GPU (no intermediate CPU transfers)\n4. Pipeline support for chaining operations efficiently\n5. Automatic fallback to CPU for small batches\n\nExpected performance on Kaggle T4x2:\n- Batch size 20+:   2-5x speedup\n- Batch size 50+:   3-10x speedup  \n- Batch size 100+:  5-15x speedup\n- Pipeline ops:     10-30x speedup (stays on GPU)\n\nTo use in run_batt.py:\n  processor = GPUBatchProcessor(batch_size=64, use_gpu=True)\n  results = processor.process_tasks_batch(tasks)\n\n======================================================================\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"'''\n# let's try to make a submission\n\nsubmission = dict()\n# iterate over all tasks\nfor key, task in tqdm.tqdm(test_challenges.items()):\n    train_inputs = [example['input'] for example in task['train']]\n    train_outputs = [example['output'] for example in task['train']]\n    hypotheses = []\n    # iterate over all programs\n    for program_string, program in programs.items():\n        try:\n            if all([program(i) == o for i, o in zip(train_inputs, train_outputs)]):\n                # remember program if it explains all training examples\n                hypotheses.append(program_string)\n        except:\n            pass\n    # select first program for making predictions\n    predictions = [example['input'] for example in task['test']]\n    if len(hypotheses) > 0:\n        print(f'found {len(hypotheses)} candidate programs for task {key}!')\n        program_string = hypotheses[0]\n        program = eval(program_string)\n        try:\n            predictions = [program(example['input']) for example in task['test']]\n        except:\n            pass\n    submission[key] = [{'attempt_1': grid, 'attempt_2': grid} for grid in predictions]\nprint(f'\\nMade guesses for {len(guesses)} tasks')\n\nwith open('submission.json', 'w') as fp:\n    json.dump(submission, fp)\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}}]}